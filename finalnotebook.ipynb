{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/averyPike/languageBiasCheck/blob/main/finalnotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta4B7Z6liHqq"
   },
   "source": [
    "# Pronoun Analysis in Literature: Gender Prediction and Frequency Comparison\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Personal pronouns are a critical element of language that can provide insights into character portrayal and gender representation in literature. This study investigates two primary questions:\n",
    "1. Is there a statistically significant difference between the frequencies of subject and object pronouns for feminine and masculine personal pronouns?\n",
    "2. Is it possible to predict the gender of a character in a story using the pronouns associated with them?\n",
    "\n",
    "To address these questions, we analyze subjective pronouns (he, she) and objective pronouns (him, her). While \"him\" is straightforward to identify as an objective pronoun, \"her\" can be either an objective pronoun or a possessive adjective. To distinguish between these usages, we implemented a function for part-of-speech (POS) tagging that identifies instances of \"her\" not followed by a noun (NN or NNS type) as objective, and the rest as possessive. We also considered possessive adjectives (his, her) and possessive pronouns (his, hers), treating \"his\" as both an adjective and a pronoun for simplicity.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Datasets\n",
    "\n",
    "We used the following literary works from Calvin's Project Gutenberg repository:\n",
    "- *Pride and Prejudice* by Jane Austen\n",
    "- *Frankenstein: Or, the Modern Prometheus* by Mary Shelley\n",
    "- *Wuthering Heights* by Emily Brontë\n",
    "\n",
    "### Technologies\n",
    "\n",
    "We utilized SparkNLP for POS-tagging and Spacy for Named-Entity Recognition (NER).\n",
    "\n",
    "### Process\n",
    "\n",
    "1. **Corpus Preparation**: Identified the corpus of interest and cleaned the text by removing periods after honorifics and hyphens.\n",
    "2. **Text Processing**: Reinserted periods to facilitate the identification of objective \"her.\" This ensured possessive adjectives \"her\" followed by nouns were correctly identified.\n",
    "3. **Pronoun Counting**: Counted instances of male and female pronouns in each sentence, associating counts with character names in a dictionary.\n",
    "4. **Dictionary Conversion**: Converted the dictionary to a list for iteration and further processing.\n",
    "5. **Character Filtering**: Excluded characters with fewer than 10 pronoun occurrences to focus on significant data points.\n",
    "6. **Gender Prediction**: Predicted the gender of each character based on pronoun counts.\n",
    "7. **Manual Verification**: The function \"manually_verify\" allowed manual verification of character gender, enhancing the accuracy of predictions.\n",
    "8. **Accuracy Calculation**: Compared true and predicted values to determine accuracy, including total accuracy, male accuracy, and female accuracy.\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "1. Given that all books in the corpus are authored by women, we hypothesized no statistical difference between the counts of subjective male pronouns, subjective female pronouns, objective male pronouns, and objective female pronouns.\n",
    "2. We hypothesized that gender prediction based on pronoun counts in relevant sentences would be more accurate than a random guess.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Character Gender Prediction\n",
    "\n",
    "Accuracy was calculated by dividing the number of correct predictions by the total number of predictions. Results were split by book to evaluate model performance across different texts. We also measured accuracy by gender. Notably, *Frankenstein* achieved 100% accuracy, likely due to its smaller cast of characters.\n",
    "\n",
    "### Pronoun Frequency Analysis\n",
    "\n",
    "We conducted statistical tests to compare the frequencies of male and female subjective and objective pronouns. Our findings indicated no statistically significant differences between the frequencies of male and female subjective pronouns, nor between male and female objective pronouns.\n",
    "\n",
    "## Conclusions and Future Research\n",
    "\n",
    "Our initial hypothesis that a corpus of entirely women authors would lead to no difference between the frequency of pronouns for each gender was supported by our model. We also had success with our predictive accuracy of gender counts. The study supports the notion that the gender of the author affects not only the frequency of respective gender pronouns, but also predictive accuracy within the corpus.\n",
    "\n",
    "This observation suggests the need for further research into pronoun usage concerning author gender. It would be irresponsible to suggest any concrete relationship between author gender and pronoun prediction accuracy from such a small corpus, but these initial findings and confirmation of our hypotheses are promising in suggesting that the gender of the author has significant influence on gender pronoun usage.\n",
    "\n",
    "With confirmation from a larger corpus and appropriate comparative analysis with a corpus of male authors, there could be considerable real-world significance to the observations made from this study. Because most published materials in English have had male authorship, this could suggest that predictive models that do not consider the ratio of authorship between genders possess inherent bias against recognition of feminine pronouns. This point contributes to linguistic arguments that a lack of representation in the development of English has obfuscated feminine viewpoints, contributing to patriarchal ideological concepts such as the “Feminine Mystique.” By analyzing this corpus, this study suggests that there is validity in further research in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "pip installs, library imports, and global vars"
   ],
   "metadata": {
    "id": "cTJDTGGvEGNe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install sparknlp\n",
    "%pip install pyspark\n",
    "%pip install spacy"
   ],
   "metadata": {
    "id": "evhlEEXBEBme",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cc6b7419-51d0-4dcc-db15-c23ea833cdaa"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sparknlp\n",
      "  Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
      "Collecting spark-nlp (from sparknlp)\n",
      "  Downloading spark_nlp-5.4.1-py2.py3-none-any.whl (579 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m579.2/579.2 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sparknlp) (1.25.2)\n",
      "Installing collected packages: spark-nlp, sparknlp\n",
      "Successfully installed spark-nlp-5.4.1 sparknlp-1.0.0\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.0/317.0 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=509b5340b72b4f0c3cb79f3e317ad33f090da941b69511634e11db2fb7bba152\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sparknlp\n",
    "import os\n",
    "import spacy\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "v2hhY8g3EBa0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsMErUvkqQHk",
    "outputId": "facd858c-898e-4c88-b88d-a3d070a02a13"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "root = os.path.dirname(os.path.realpath('cormac.ipynb'))\n",
    "pipeline = PretrainedPipeline(\"explain_document_ml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "get corpus"
   ],
   "metadata": {
    "id": "xjqBxlqjE0An"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "book_list = ['pg1342.txt', 'pg768.txt', 'pg84.txt']\n",
    "# curl each book\n",
    "for book in book_list:\n",
    "  !curl \"https://raw.githubusercontent.com/cd-public/books/main/{book}\" -o {book}"
   ],
   "metadata": {
    "id": "oNegfGi-ERfi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aae0d9dc-7426-4e57-9eec-27b5f480ebba"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  739k  100  739k    0     0  1620k      0 --:--:-- --:--:-- --:--:-- 1618k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  665k  100  665k    0     0  1478k      0 --:--:-- --:--:-- --:--:-- 1481k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  430k  100  430k    0     0  1371k      0 --:--:-- --:--:-- --:--:-- 1372k\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdKebnQ0sJEU"
   },
   "outputs": [],
   "source": [
    "def read_txt(filename = 'pg1342.txt'):\n",
    "  file = open(root + '/' + filename,\"r\")\n",
    "  return file.read()\n",
    "pride = read_txt()\n",
    "frank = read_txt('pg84.txt')\n",
    "wuther = read_txt('pg768.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtnweMxH6sSZ"
   },
   "outputs": [],
   "source": [
    "pronouns = ['he','him','his','she','her','hers']\n",
    "def pronoun_check(x):\n",
    "  '''\n",
    "  x = list of tuples\n",
    "  returns list of tuples with only pronouns\n",
    "  '''\n",
    "  pro_toks = []\n",
    "  male_count, female_count = 0, 0\n",
    "  for i in x:\n",
    "    flag = False\n",
    "    last_pos = ''\n",
    "    last_tok = ''\n",
    "    for y in i:\n",
    "      if flag:\n",
    "        if pos != 'NN' and pos != 'NNS' and token == '.':\n",
    "          female_count += 1\n",
    "          flag = False\n",
    "          continue\n",
    "      token, pos = y\n",
    "      token = token.lower()\n",
    "      # Magic (identifies instances of 'her' that are objective)\n",
    "      if token in pronouns:\n",
    "        if token == 'her':\n",
    "          flag = True\n",
    "          last_pos = pos\n",
    "          last_tok = token\n",
    "        else:\n",
    "            if token == 'he' or token == 'him':\n",
    "              male_count += 1\n",
    "            else:\n",
    "              female_count += 1\n",
    "\n",
    "  return male_count, female_count\n",
    "\n",
    "#def count_pronouns(x):"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def count_subObjPos(x):\n",
    "  '''\n",
    "  x = list of tuples\n",
    "  returns count of instances of each type of pronoun\n",
    "  '''\n",
    "  pro_toks = []\n",
    "  male_countSub, female_countSub, male_countObj, female_countObj, male_countPos, female_countPos = 0, 0, 0, 0, 0, 0\n",
    "  male_count, female_count = 0, 0\n",
    "  for i in x:\n",
    "    flag = False\n",
    "    last_pos = ''\n",
    "    last_tok = ''\n",
    "    for y in i:\n",
    "      if flag:\n",
    "        if pos != 'NN' and pos != 'NNS' and token == '.':\n",
    "          female_countObj += 1\n",
    "          flag = False\n",
    "        else:\n",
    "          female_countPos += 1\n",
    "          flag = False\n",
    "          continue\n",
    "      token, pos = y\n",
    "      token = token.lower()\n",
    "      # Magic (identifies instances of 'her' that are objective)\n",
    "      if token in pronouns:\n",
    "        if token == 'her':\n",
    "          flag = True\n",
    "          last_pos = pos\n",
    "          last_tok = token\n",
    "        else:\n",
    "            if token == 'he':\n",
    "              male_countSub += 1\n",
    "            elif token == 'him':\n",
    "              male_countObj += 1\n",
    "            elif token == 'she':\n",
    "              female_countSub += 1\n",
    "            elif token == 'hers':\n",
    "              female_countPos += 1\n",
    "            elif token == 'his':\n",
    "              male_countPos += 1\n",
    "  male_count = male_countSub + male_countObj + male_countPos\n",
    "  female_count = female_countSub + female_countObj + female_countPos\n",
    "\n",
    "  return male_count, female_count, male_countSub, female_countSub, male_countObj, female_countObj, male_countPos, female_countPos"
   ],
   "metadata": {
    "id": "KqkHK7mKQqDi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-XHb5PVqX8V"
   },
   "outputs": [],
   "source": [
    "def book_cleaner(text):\n",
    "  text = text.replace('Mr.','Mr')\n",
    "  text = text.replace('Mrs.','Mrs')\n",
    "  text = text.replace('Ms.','Ms')\n",
    "  text = text.replace('Dr.','Dr')\n",
    "  text = text.replace('_','')\n",
    "  text = text.replace('-','')\n",
    "  # text = text.replace('\\n', ' ')\n",
    "  # text = text.replace('\"\"',' ')\n",
    "  # text = text.replace('  ',' ')\n",
    "  # # text = text.lower()\n",
    "  return text\n",
    "\n",
    "pridesplit = book_cleaner(pride).split('.')\n",
    "franksplit = book_cleaner(frank).split('.')\n",
    "wuthersplit = book_cleaner(wuther).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T9ebsbn15pj"
   },
   "outputs": [],
   "source": [
    "def period(sentences):\n",
    "  sent_list = []\n",
    "  for sent in sentences:\n",
    "    sent = sent+'.'\n",
    "    sent_list.append(sent)\n",
    "  return sent_list\n",
    "pridesplit = period(pridesplit)\n",
    "franksplit = period(franksplit)\n",
    "wuthersplit = period(wuthersplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "for each sentence, find all people mentiond. Add the number of male and female pronounce counted in the sentence to a dict where the key is the persons name."
   ],
   "metadata": {
    "id": "g6c2jw4Fv3D6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvwfjdrhtni8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6132bf09-cf5d-4575-d317-b338199359db"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5516/5516 [11:31<00:00,  7.98it/s]\n",
      "100%|██████████| 3129/3129 [02:44<00:00, 18.98it/s]\n",
      "100%|██████████| 4922/4922 [08:06<00:00, 10.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def book_person_dict_builder(split_book):\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  personGenderCount = {} # person: malePronounCount, femalePronounCount\n",
    "  # for sent in pridesplit:\n",
    "  for sent in tqdm(split_book): # for each sentence in pride\n",
    "    doc = nlp(sent) # create the spacy doc\n",
    "    people = []\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_ == 'PERSON':\n",
    "        people.append(ent.text) # get all people entities\n",
    "    if len(people) == 0: # if there aren't any people\n",
    "      continue # move onto next sentence\n",
    "    annoted_sent = pipeline.annotate(sent) # annotate the sentence with parts of speach\n",
    "    tok_tag = [(annoted_sent['token'], annoted_sent['pos'])] # list of tuples (token, part of speech)\n",
    "    zips = [list(zip(tt[0],tt[1])) for tt in tok_tag] # zip\n",
    "    male_count, female_count,_,_,_,_,_,_ = count_subObjPos(zips) # get male/female PN counts\n",
    "    for person in people: # for each person in the sentence\n",
    "      try: # if they already exist in the the dict\n",
    "        # add the old counts to the new\n",
    "        person_male_count = personGenderCount[person][0]\n",
    "        person_female_count = personGenderCount[person][1]\n",
    "        personGenderCount[person] = person_male_count + male_count, person_female_count + female_count\n",
    "      except KeyError: # if they don't exist in the dict, add them\n",
    "        personGenderCount[person] = male_count, female_count\n",
    "\n",
    "  return personGenderCount\n",
    "\n",
    "pride_dict = book_person_dict_builder(pridesplit)\n",
    "frank_dict = book_person_dict_builder(franksplit)\n",
    "wuther_dict = book_person_dict_builder(wuthersplit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "convert dict to list so we can itterate through it"
   ],
   "metadata": {
    "id": "_9g3evK2vtSU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def dict_to_list(dict):\n",
    "  # convert personGenderCount to list\n",
    "  out = []\n",
    "  for key, value in dict.items():\n",
    "    m_count, f_count = value[0], value[1]\n",
    "    if m_count + f_count < 10:\n",
    "      continue\n",
    "    out.append([key, value[0], value[1]])\n",
    "  return out\n",
    "\n",
    "personGenderList_pride = dict_to_list(pride_dict)\n",
    "personGenderList_frank = dict_to_list(frank_dict)\n",
    "personGenderList_wuther = dict_to_list(wuther_dict)\n"
   ],
   "metadata": {
    "id": "naMWykihBnWq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "manually verify each persons gender (do not presume to know unless prefaced by Mr, Mrs, ... etc)"
   ],
   "metadata": {
    "id": "zNgh7Q22vbEw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def manual_verify(personGenderList, book):\n",
    "  out_list = [] # will consist of [name, prediction, actual]\n",
    "  try:\n",
    "    old_out_file = pd.read_csv(book + '_output' + '.csv')\n",
    "  except:\n",
    "    old_out_file = []\n",
    "  for person in personGenderList:\n",
    "    name, male_count, female_count = person[0], person[1], person[2]\n",
    "    prediciton = 1 if male_count > female_count else 0\n",
    "    if len(old_out_file) > 0:\n",
    "      if name in old_out_file['name'].values:\n",
    "        actual = old_out_file[old_out_file['name'] == name]['actual'].values[0]\n",
    "    else:\n",
    "      actual = input('Is ' + name + ': ' + str(prediciton) + '? ') # 1 for M, 0 for F, -1 for other\n",
    "    out_list.append([name, prediciton, actual])\n",
    "  return out_list\n",
    "\n",
    "out_list_pride = manual_verify(personGenderList_pride, 'pride')\n",
    "out_list_frank = manual_verify(personGenderList_frank, 'frank')\n",
    "out_list_wuther = manual_verify(personGenderList_wuther, 'wuther')"
   ],
   "metadata": {
    "id": "yVqnpHSeo2YX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "outputId": "213c1d2e-c8ac-4cab-ee22-0dcc31c28ec2"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Mary: 0? Yes\n",
      "Is Austen: 0? Yes\n",
      "Is Wickham: 1? Yes\n",
      "Is Jane: 0? Yes\n",
      "Is Darcy: 1? No\n",
      "Is Elizabeth: 0? No\n",
      "Is Bennet: 0? \n",
      "Is Collins: 1? no\n",
      "Is Mrs\n",
      "Bennet: 1? no\n",
      "Is Mrs Bennet: 0? no\n",
      "Is Mr Collins: 1? no\n",
      "Is Lady Catherine de Bourgh: 1? no\n",
      "Is Lady Catherine: 0? o\n",
      "Is Kitty: 0? yes\n",
      "Is Mrs Bennet’s: 0? \n",
      "Is Bingleys: 0? \n",
      "Is Pemberley: 0? \n",
      "Is Mr Bingley: 0? \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-d86a01a7a3fa>\u001B[0m in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mout_list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0mout_list_pride\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmanual_verify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpersonGenderList_pride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pride'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0mout_list_frank\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmanual_verify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpersonGenderList_frank\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'frank'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0mout_list_wuther\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmanual_verify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpersonGenderList_wuther\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wuther'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-14-d86a01a7a3fa>\u001B[0m in \u001B[0;36mmanual_verify\u001B[0;34m(personGenderList, book)\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mactual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_out_file\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mold_out_file\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'actual'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m       \u001B[0mactual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Is '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m': '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediciton\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'? '\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# 1 for M, 0 for F, -1 for other\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m     \u001B[0mout_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprediciton\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mactual\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mout_list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001B[0m in \u001B[0;36mraw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m    849\u001B[0m                 \u001B[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    850\u001B[0m             )\n\u001B[0;32m--> 851\u001B[0;31m         return self._input_request(str(prompt),\n\u001B[0m\u001B[1;32m    852\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_parent_ident\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    853\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_parent_header\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001B[0m in \u001B[0;36m_input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m    893\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    894\u001B[0m                 \u001B[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 895\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Interrupted by user\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    896\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    897\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Invalid Message:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexc_info\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "save to file"
   ],
   "metadata": {
    "id": "rs1FImJUvJik"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def save_books_person_dict(out_list, book):\n",
    "  df = pd.DataFrame(out_list, columns=['name', 'prediction', 'actual'])\n",
    "  df = df.astype({'actual': int})\n",
    "  df.to_csv(book + '_output' + '.csv', index=False)\n",
    "  return df\n",
    "\n",
    "df_pride = save_books_person_dict(out_list_pride, 'pride')\n",
    "df_frank = save_books_person_dict(out_list_frank, 'frank')\n",
    "df_wuther = save_books_person_dict(out_list_wuther, 'wuther')"
   ],
   "metadata": {
    "id": "jGceb3rvrGRH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "outputId": "0ec9d3be-9a78-44a2-a75b-be8241b5c6a3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'out_list_pride' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-995e9ac306d0>\u001B[0m in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mdf_pride\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msave_books_person_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_list_pride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pride'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0mdf_frank\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msave_books_person_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_list_frank\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'frank'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mdf_wuther\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msave_books_person_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_list_wuther\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wuther'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'out_list_pride' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# df[df['name'] == 'ellen'][actual] = 0\n",
    "df_wuther.loc[df_wuther['name'] == 'Ellen', 'actual'] = 0"
   ],
   "metadata": {
    "id": "t7OOos5Jdl6n"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculation of Acurracy"
   ],
   "metadata": {
    "id": "2PUyv7zyvPvM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy_calc(df, book):\n",
    "  print(f'Book: {book}')\n",
    "  # calculate accuracy, when predicted == actual that's good\n",
    "  accuracy = (df['prediction'] == df['actual']).sum() / len(df)\n",
    "  print(f'Total Accuracy: {round(accuracy*100,2)}%')\n",
    "  # female accuracy\n",
    "  female_df = df[df['actual'] == 0]\n",
    "  female_accuracy = (female_df['prediction'] == female_df['actual']).sum() / len(female_df)\n",
    "  print(f'Female Accuracy: {round(female_accuracy*100,2)}%')\n",
    "  # male accuracy\n",
    "  male_df = df[df['actual'] == 1]\n",
    "  male_accuracy = (male_df['prediction'] == male_df['actual']).sum() / len(male_df)\n",
    "  print(f'Male Accuracy: {round(male_accuracy*100,2)}%\\n')\n",
    "\n",
    "accuracy_calc(df_pride, 'Pride and Prejudice')\n",
    "accuracy_calc(df_frank, 'Frakenstein')\n",
    "accuracy_calc(df_wuther, 'Wuthering Heights')\n",
    "df_all = pd.concat([df_pride, df_frank, df_wuther])\n",
    "accuracy_calc(df_all, 'All Books')"
   ],
   "metadata": {
    "id": "-5rqFMfortjS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "books = [\n",
    "    accuracy_calc(df_pride, 'Pride and Prejudice'),\n",
    "    accuracy_calc(df_frank, 'Frankenstein'),\n",
    "    accuracy_calc(df_wuther, 'Wuthering Heights')\n",
    "]\n",
    "\n",
    "df_all = pd.concat([df_pride, df_frank, df_wuther])\n",
    "books.append(accuracy_calc(df_all, 'All Books'))\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Accuracy of Predictions for Each Book')\n",
    "\n",
    "for i, (book, total_acc, female_acc, male_acc) in enumerate(books):\n",
    "    ax = axs[i//2, i%2]\n",
    "    ax.bar(['Total', 'Female', 'Male'], [accuracy, female_accuracy, male_accuracy], color=['green', 'purple', 'orange'])\n",
    "    ax.set_title(book)\n",
    "    ax.set_ylim([0, 100])\n",
    "    for index, value in enumerate([total_acc, female_acc, male_acc]):\n",
    "        ax.text(index, value + 1, f'{value:.2f}%', ha='center')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "B8r5OqhsmTUg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model was very good at predicting female characters, terrible at predicting males. This could be a result of the main subjectmatter of this book being women. A comparative project could be done to analyze whether there is correlation between gender recognition accuracy and the gender of the author."
   ],
   "metadata": {
    "id": "VMXwk-u8yEoS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Pronoun Frequencies\n",
    "\n",
    "#xs = ['1', '2', '3']\n",
    "#s = ''.join(xs)\n",
    "\n",
    "books = [' '.join(pridesplit), ' '.join(franksplit,),  ' '.join(wuthersplit)]\n",
    "\n",
    "def analyze_pronouns(corpus):\n",
    "    \"\"\"\n",
    "    corpus: list of documents where each document is a list of tuples\n",
    "            with (token, pos) representing token and its part-of-speech.\n",
    "    returns: dictionary with t-test results for male and female pronoun counts.\n",
    "    \"\"\"\n",
    "    male_countSub_list = []\n",
    "    female_countSub_list = []\n",
    "    male_countObj_list = []\n",
    "    female_countObj_list = []\n",
    "\n",
    "\n",
    "    for doc in corpus:\n",
    "      annoted_sent = pipeline.annotate(doc) # annotate the sentence with parts of speach\n",
    "      tok_tag = [(annoted_sent['token'], annoted_sent['pos'])] # list of tuples (token, part of speech)\n",
    "      zips = [list(zip(tt[0],tt[1])) for tt in tok_tag]\n",
    "      male_count, female_count, male_countSub, female_countSub, male_countObj, female_countObj, male_countPos, female_countPos = count_subObjPos(zips)\n",
    "\n",
    "      male_countSub_list.append(male_countSub)\n",
    "      female_countSub_list.append(female_countSub)\n",
    "      male_countObj_list.append(male_countObj)\n",
    "      female_countObj_list.append(female_countObj)\n",
    "\n",
    "    # Perform t-tests\n",
    "    ttest_results = {}\n",
    "\n",
    "    ttest_results['male_sub_vs_female_sub'] = stats.ttest_ind(male_countSub_list, female_countSub_list, equal_var=False)\n",
    "    ttest_results['male_obj_vs_female_obj'] = stats.ttest_ind(male_countObj_list, female_countObj_list, equal_var=False)\n",
    "\n",
    "    print()\n",
    "\n",
    "    return ttest_results"
   ],
   "metadata": {
    "id": "zyhZEmUvhLfB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ttest = analyze_pronouns(books)"
   ],
   "metadata": {
    "id": "RWq6UIItjSDW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ttest"
   ],
   "metadata": {
    "id": "FYV3DCHhjbq-"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
